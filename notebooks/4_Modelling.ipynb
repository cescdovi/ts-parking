{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelado de Series Temporales de Aparcamientos usando RNNs y optimizaci√≥n con Optuna\n",
    "En este notebook se abordar√° un problema de predicci√≥n de series temporales utilizando datos de disponibilidad de parkings.\n",
    "\n",
    "Flujo de trabajo:\n",
    "1. Agrupaci√≥n de los datos por id de aparcamiento (idAparcamiento), para tratar cada parking como una serie temporal independiente.\n",
    "2. Divisi√≥n del conjunto de datos en tres subconjuntos: entrenamiento (train), validaci√≥n (val) y prueba (test).\n",
    "3. Entrenamiento de modelos de redes neuronales recurrentes simples (vanilla):\n",
    " - Vanilla RNN\n",
    " - Vanilla GRU\n",
    " - Vanilla LSTM\n",
    "4. Ajuste de hiperpar√°metros utilizando Optuna para encontrar la configuraci√≥n √≥ptima en cada tipo de modelo.\n",
    "5. Comparaci√≥n de los resultados de rendimiento entre los distintos modelos utilizando m√©tricas apropiadas.\n",
    "\n",
    "El objetivo final es evaluar qu√© arquitectura ofrece mejores resultados para este tipo de datos y tarea de predicci√≥n.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.regression import MeanAbsoluteError, MeanSquaredError, MeanAbsolutePercentageError, R2Score\n",
    "\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "HIDDEN_SIZE = 32\n",
    "NUM_LAYERS = 1\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idAparcamiento</th>\n",
       "      <th>PlazasTotales</th>\n",
       "      <th>PlazasDisponibles</th>\n",
       "      <th>PorcPlazasDisponibles</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-02-03 10:00:00</th>\n",
       "      <td>6</td>\n",
       "      <td>372.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>16.129032</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-03 11:00:00</th>\n",
       "      <td>6</td>\n",
       "      <td>372.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>12.903226</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-03 12:00:00</th>\n",
       "      <td>6</td>\n",
       "      <td>372.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>17.741935</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-03 13:00:00</th>\n",
       "      <td>6</td>\n",
       "      <td>372.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>31.989247</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-03 14:00:00</th>\n",
       "      <td>6</td>\n",
       "      <td>372.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-05 03:00:00</th>\n",
       "      <td>78</td>\n",
       "      <td>464.0</td>\n",
       "      <td>355.0</td>\n",
       "      <td>76.508621</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-05 04:00:00</th>\n",
       "      <td>78</td>\n",
       "      <td>464.0</td>\n",
       "      <td>355.0</td>\n",
       "      <td>76.508621</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-05 05:00:00</th>\n",
       "      <td>78</td>\n",
       "      <td>464.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>76.724138</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-05 06:00:00</th>\n",
       "      <td>78</td>\n",
       "      <td>464.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>76.293103</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-05 07:00:00</th>\n",
       "      <td>78</td>\n",
       "      <td>464.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>71.551724</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149395 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     idAparcamiento  PlazasTotales  PlazasDisponibles  \\\n",
       "timestamp                                                               \n",
       "2023-02-03 10:00:00               6          372.0               60.0   \n",
       "2023-02-03 11:00:00               6          372.0               48.0   \n",
       "2023-02-03 12:00:00               6          372.0               66.0   \n",
       "2023-02-03 13:00:00               6          372.0              119.0   \n",
       "2023-02-03 14:00:00               6          372.0              155.0   \n",
       "...                             ...            ...                ...   \n",
       "2025-03-05 03:00:00              78          464.0              355.0   \n",
       "2025-03-05 04:00:00              78          464.0              355.0   \n",
       "2025-03-05 05:00:00              78          464.0              356.0   \n",
       "2025-03-05 06:00:00              78          464.0              354.0   \n",
       "2025-03-05 07:00:00              78          464.0              332.0   \n",
       "\n",
       "                     PorcPlazasDisponibles  year  month  day  weekday  \n",
       "timestamp                                                              \n",
       "2023-02-03 10:00:00              16.129032  2023      2    3        4  \n",
       "2023-02-03 11:00:00              12.903226  2023      2    3        4  \n",
       "2023-02-03 12:00:00              17.741935  2023      2    3        4  \n",
       "2023-02-03 13:00:00              31.989247  2023      2    3        4  \n",
       "2023-02-03 14:00:00              41.666667  2023      2    3        4  \n",
       "...                                    ...   ...    ...  ...      ...  \n",
       "2025-03-05 03:00:00              76.508621  2025      3    5        2  \n",
       "2025-03-05 04:00:00              76.508621  2025      3    5        2  \n",
       "2025-03-05 05:00:00              76.724138  2025      3    5        2  \n",
       "2025-03-05 06:00:00              76.293103  2025      3    5        2  \n",
       "2025-03-05 07:00:00              71.551724  2025      3    5        2  \n",
       "\n",
       "[149395 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/processed/data_processed.csv\")\n",
    "\n",
    "#convertir a indice\n",
    "df.set_index(\"timestamp\", inplace= True)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Divisi√≥n del conjunto de datos\n",
    "\n",
    "Realizamos la divisi√≥n del conjunto de datos, agrupando por `idAparcamiento`. \n",
    "La idea es que el conjunto de **test sea com√∫n** para todos los aparcamientos, correspondiente al **√∫ltimo 10% del rango temporal total** del dataset. \n",
    "\n",
    "El resto de los datos disponibles para cada parking se dividen en:\n",
    "\n",
    "- **Entrenamiento (train)**: el primer 85% de los datos previos al test.\n",
    "- **Validaci√≥n (val)**: el √∫ltimo 15% de los datos previos al test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "# 1. Calcular el rango temporal global\n",
    "fecha_min_global = df.index.min()\n",
    "fecha_max_global = df.index.max()\n",
    "rango_total = fecha_max_global - fecha_min_global\n",
    "\n",
    "# 2. Calcular el inicio del conjunto de test (√∫ltimo 10% del rango)\n",
    "test_ratio = 0.10\n",
    "test_duration = timedelta(seconds=rango_total.total_seconds() * test_ratio)\n",
    "test_start = fecha_max_global - test_duration\n",
    "\n",
    "# 3. Diccionarios para almacenar los splits\n",
    "train_dict = {}\n",
    "val_dict = {}\n",
    "test_dict = {}\n",
    "\n",
    "val_ratio = 0.1  # del conjunto anterior al test\n",
    "\n",
    "# 4. Divisi√≥n por parking\n",
    "for parking_id, group in df.groupby(\"idAparcamiento\"):\n",
    "    group = group.sort_index()\n",
    "    \n",
    "    # Split basado en el corte global\n",
    "    test_set = group[group.index >= test_start]\n",
    "    remaining = group[group.index < test_start]\n",
    "\n",
    "    # Dividir en train y val\n",
    "    val_size = int(len(remaining) * val_ratio)\n",
    "    val_set = remaining.iloc[-val_size:]\n",
    "    train_set = remaining.iloc[:-val_size]\n",
    "    \n",
    "    # Guardar resultados\n",
    "    train_dict[parking_id] = train_set\n",
    "    val_dict[parking_id] = val_set\n",
    "    test_dict[parking_id] = test_set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Definir `Dataset` de pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, input_window, output_window, feature_cols, target_col):\n",
    "        \"\"\"\n",
    "        data: DataFrame que contiene los datos de la serie temporal\n",
    "        input_window: n¬∫ de pasos de tiempo en la secuencia de entrada\n",
    "        output_window: n¬∫ de pasos de tiempo a predecir\n",
    "        feature_cols: lista de nombres de columnas que se usan como caracter√≠stcas\n",
    "        target_col: nombre de la variable a predecir\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.input_window = input_window\n",
    "        self.output_window = output_window\n",
    "        self.feature_cols = feature_cols\n",
    "        self.target_cols = target_col\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Funci√≥n que devuele el n¬∫ de datos del Dataset\n",
    "        \"\"\"\n",
    "        return len(self.data) - self.input_window - self.output_window + 1 #\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Funci√≥n que devuelve un dato a partir de un √≠ndice\n",
    "        \"\"\"\n",
    "        X = self.data[idx: idx + self.input_window][self.feature_cols].values\n",
    "        Y = self.data[idx + self.input_window: idx + self.input_window + self.output_window][self.target_cols].values\n",
    "        \n",
    "        X_tensor = torch.tensor(X, dtype= torch.float32)\n",
    "        Y_tensor = torch.tensor(Y, dtype= torch.float32)\n",
    "\n",
    "        return X_tensor, Y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['PorcPlazasDisponibles']  \n",
    "target_col = 'PorcPlazasDisponibles'     \n",
    "input_window = 24         # N√∫mero de pasos de tiempo en la secuencia de entrada\n",
    "output_window = 1         # N√∫mero de pasos de tiempo a predecir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que tenemos una serie temporal por parking, construimos un diccionario, donde las claves son los identificadores de parkings y los valores son Daatasets de tipo `torch.utils.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = {\n",
    "    pid: TimeSeriesDataset(df, input_window, output_window, feature_cols, target_col=target_col)\n",
    "    for pid, df in train_dict.items()\n",
    "}\n",
    "val_datasets = {\n",
    "    pid: TimeSeriesDataset(df, input_window, output_window, feature_cols, target_col=target_col)\n",
    "    for pid, df in val_dict.items()\n",
    "}\n",
    "test_datasets = {\n",
    "    pid: TimeSeriesDataset(df, input_window, output_window, feature_cols, target_col=target_col)\n",
    "    for pid, df in test_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id parking:  6\n",
      "longitud:  13908\n",
      "id parking:  7\n",
      "longitud:  13908\n",
      "id parking:  8\n",
      "longitud:  22507\n",
      "id parking:  13\n",
      "longitud:  13908\n",
      "id parking:  34\n",
      "longitud:  9310\n",
      "id parking:  75\n",
      "longitud:  22505\n",
      "id parking:  77\n",
      "longitud:  10735\n",
      "id parking:  78\n",
      "longitud:  7454\n"
     ]
    }
   ],
   "source": [
    "for pid, df in train_datasets.items():\n",
    "    print(\"id parking: \", pid)\n",
    "    print(\"longitud: \", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Crear `DataLoaders` a partir de `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear los `DataLoader`, seguimos el mismo criterio, es decir, crear un diccionario de DataLoaders, en el que cada iteraci√≥n nos devuelve un batch de datos para cada parking.\n",
    "- Cada batch de datos debe tener dimensi√≥n: `(batch_size, window_size, n_features)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloaders = {\n",
    "    pid: DataLoader(ts_dataset, batch_size = 32, shuffle = True) #(batch_size, window_size, n_features)\n",
    "    for pid, ts_dataset in train_datasets.items()\n",
    "}\n",
    "\n",
    "val_dataloaders = {\n",
    "    pid: DataLoader(ts_dataset, batch_size = 32, shuffle = True) #(batch_size, window_size, n_features)\n",
    "    for pid, ts_dataset in val_datasets.items()\n",
    "}\n",
    "\n",
    "test_dataloaders = {\n",
    "    pid: DataLoader(ts_dataset, batch_size = 64) #(batch_size, window_size, n_features)\n",
    "    for pid, ts_dataset in test_datasets.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id parking 6\n",
      "Dimensi√≥n del primer batch de datos: torch.Size([32, 24, 1])\n",
      "Dimensi√≥n del primer batch de etiquetas:  torch.Size([32, 1])\n",
      "\n",
      "\n",
      "id parking 7\n",
      "Dimensi√≥n del primer batch de datos: torch.Size([32, 24, 1])\n",
      "Dimensi√≥n del primer batch de etiquetas:  torch.Size([32, 1])\n",
      "\n",
      "\n",
      "id parking 8\n",
      "Dimensi√≥n del primer batch de datos: torch.Size([32, 24, 1])\n",
      "Dimensi√≥n del primer batch de etiquetas:  torch.Size([32, 1])\n",
      "\n",
      "\n",
      "id parking 13\n",
      "Dimensi√≥n del primer batch de datos: torch.Size([32, 24, 1])\n",
      "Dimensi√≥n del primer batch de etiquetas:  torch.Size([32, 1])\n",
      "\n",
      "\n",
      "id parking 34\n",
      "Dimensi√≥n del primer batch de datos: torch.Size([32, 24, 1])\n",
      "Dimensi√≥n del primer batch de etiquetas:  torch.Size([32, 1])\n",
      "\n",
      "\n",
      "id parking 75\n",
      "Dimensi√≥n del primer batch de datos: torch.Size([32, 24, 1])\n",
      "Dimensi√≥n del primer batch de etiquetas:  torch.Size([32, 1])\n",
      "\n",
      "\n",
      "id parking 77\n",
      "Dimensi√≥n del primer batch de datos: torch.Size([32, 24, 1])\n",
      "Dimensi√≥n del primer batch de etiquetas:  torch.Size([32, 1])\n",
      "\n",
      "\n",
      "id parking 78\n",
      "Dimensi√≥n del primer batch de datos: torch.Size([32, 24, 1])\n",
      "Dimensi√≥n del primer batch de etiquetas:  torch.Size([32, 1])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pid, dloader in train_dataloaders.items():\n",
    "    print(\"id parking\", pid)\n",
    "    for batch in dloader:\n",
    "        print(\"Dimensi√≥n del primer batch de datos:\", batch[0].shape)\n",
    "        print(\"Dimensi√≥n del primer batch de etiquetas: \", batch[1].shape)\n",
    "        break\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Definir Modelos (`RNN`,`GRU`,`LSTM`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Clase que implementa una RNN vanilla: 1 capa, 1 neurona por capa oculta\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.rnn = nn.RNN(input_size = self.input_size,\n",
    "                          hidden_size = self.hidden_size,\n",
    "                          num_layers = self.num_layers,\n",
    "                          batch_first = True)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "    \n",
    "    def forward(self, x, h0 = None):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size_ = x.size(0)\n",
    "        #si no se pasa el primer estado oculto, lo inicializamos con 0\n",
    "        if h0 is None:\n",
    "            h0 = torch.zeros(self.num_layers, batch_size_, self.hidden_size).to(x.device)\n",
    "        \n",
    "        #all_hidden_states (output) --> todos los estados ocultos (batch_size, seq_length, hidden_size)\n",
    "        #last_hidden_state (h_n) --> ultimo estado oculto (num_layers, batch_size, hidden_size)\n",
    "        all_hidden_states, last_hidden_state = self.rnn(x, h0)\n",
    "\n",
    "        last_hidde_state_of_last_layer = last_hidden_state[-1]\n",
    "        \n",
    "        #si no pasamos por una capa densa, el modelo no hace la prediccion\n",
    "        pred = self.fc(last_hidde_state_of_last_layer)  # (batch_size, output_size)\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que no hay inconsistencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "model = VanillaRNN(input_size=1, hidden_size=64, output_size=1, num_layers=1)\n",
    "x = torch.randn(32, 24, 1)  # (batch_size, sequence_length, num_features)\n",
    "y  = model(x)\n",
    "print(y.shape)  # ‚Üí (32, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VanillaLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x, h0=None, c0=None):\n",
    "        batch_size_ = x.size(0)\n",
    "        #si no se pasa el primer h0 o c0, lo inicializamos con 0\n",
    "        if h0 is None:\n",
    "            h0 = torch.zeros(self.num_layers, batch_size_, self.hidden_size).to(x.device)\n",
    "        if c0 is None:\n",
    "            c0 = torch.zeros(self.num_layers, batch_size_, self.hidden_size).to(x.device)\n",
    "\n",
    "        all_hidden_states, (last_hidden_state, last_cell_memory) = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Usar el √∫ltimo estado oculto de la √∫ltima capa\n",
    "        pred = self.fc(last_hidden_state[-1])  # Shape: (batch_size, output_size)\n",
    "        \n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que no hay inconsistencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "model = VanillaLSTM(input_size=1, hidden_size=64, output_size=1, num_layers=1)\n",
    "x = torch.randn(32, 24, 1)  # (batch_size, sequence_length, num_features)\n",
    "y  = model(x)\n",
    "print(y.shape)  #(32, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers = 1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.gru = nn.GRU(input_size = self.input_size,\n",
    "                          hidden_size = self.hidden_size,\n",
    "                          num_layers = self.num_layers,\n",
    "                          batch_first= True)\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "    \n",
    "    def forward(self, x, h0=None):\n",
    "        \n",
    "        batch_size_ = x.size(0)\n",
    "        \n",
    "        ##si no se pasa el primer h0, lo inicializamos con 0\n",
    "        if h0 is None:\n",
    "            h0 = torch.zeros(self.num_layers, batch_size_, self.hidden_size).to(x.device)\n",
    "        \n",
    "        #all_hidden_states (output) --> todos los estados ocultos (batch_size, seq_length, hidden_size)\n",
    "        #last_hidden_state (h_n) --> ultimo estado oculto (num_layers, batch_size, hidden_size)\n",
    "        all_hidden_states, last_hidden_state = self.gru(x, h0)\n",
    "\n",
    "        last_hidde_state_of_last_layer = last_hidden_state[-1]\n",
    "        \n",
    "        #si no pasamos por una capa densa, el modelo no hace la prediccion\n",
    "        pred = self.fc(last_hidde_state_of_last_layer)  # (batch_size, output_size)\n",
    "\n",
    "        return pred  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "model = VanillaGRU(input_size=1, hidden_size=64, output_size=1, num_layers=1)\n",
    "x = torch.randn(32, 24, 1)  # (batch_size, sequence_length, num_features)\n",
    "y  = model(x)\n",
    "print(y.shape)  #(32, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Definir m√©tricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MetricCollection({\n",
    "    \"MAE\": MeanAbsoluteError(),\n",
    "    \"MSE\": MeanSquaredError(),\n",
    "    \"RMSE\": MeanSquaredError(squared = False),\n",
    "    \"MAPE\": MeanAbsolutePercentageError()\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Instanciar modelos, optimizador, funci√≥n de coste y learning rate scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üí° Estructura de entrenamiento modular por parking y arquitectura\n",
    "\n",
    "Dado que tenemos:\n",
    "\n",
    "- Un diccionario de datasets por `idAparcamiento`\n",
    "- Tres arquitecturas distintas: `RNN`, `LSTM`, y `GRU`\n",
    "\n",
    "Cada modelo se entrena **por separado para cada parking**, permitiendo una evaluaci√≥n m√°s precisa por serie temporal individual.\n",
    "\n",
    "Para hacerlo **flexible y escalable**, organizamos todos los componentes en **diccionarios anidados**. Cada uno almacena los elementos necesarios para entrenar y validar por `idAparcamiento` y por tipo de modelo:\n",
    "\n",
    "- `models[\"rnn\"][pid]`, `models[\"lstm\"][pid]`, etc.\n",
    "- `optimizers[\"gru\"][pid]`, `criterions[\"lstm\"][pid]`, etc.\n",
    "- `schedulers[\"rnn\"][pid]` para aplicar estrategias de LR por modelo\n",
    "\n",
    "Esto nos permite:\n",
    "\n",
    "- Entrenar m√∫ltiples arquitecturas de forma aislada y comparable  \n",
    "- Hacer tuning o validaci√≥n cruzada por parking  \n",
    "- Comparar vanilla vs. modelos optimizados con Optuna  \n",
    "- Guardar y cargar modelos individuales por ID\n",
    "\n",
    "Esta estructura es especialmente √∫til en contextos donde las series tienen longitudes o comportamientos distintos, como ocurre con los parkings reales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_output_size(dataloader):\n",
    "    \"\"\"\n",
    "    Extrae el n√∫mero de features de entrada y el tama√±o de salida\n",
    "    desde un batch del DataLoader.\n",
    "    \n",
    "    Retorna:\n",
    "        input_size: int (n√∫mero de features por paso temporal)\n",
    "        output_size: int (dimensi√≥n de la predicci√≥n por muestra)\n",
    "    \"\"\"\n",
    "    # Tomamos el primer batch del dataloader\n",
    "    batch = next(iter(dataloader))\n",
    "    x, y = batch\n",
    "    input_size = x.shape[-1]   # √∫ltima dimensi√≥n de x --> n_features\n",
    "    output_size = y.shape[-1]  # √∫ltima dimensi√≥n de y --> n_outputs\n",
    "    return input_size, output_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size (n_features): 1\n",
      "Output size: 1\n"
     ]
    }
   ],
   "source": [
    "dloader = train_dataloaders[6]\n",
    "INPUT_SIZE, OUTPUT_SIZE = get_input_output_size(dloader)\n",
    "\n",
    "print(\"Input size (n_features):\", INPUT_SIZE)\n",
    "print(\"Output size:\", OUTPUT_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Definir estructuras adecuadas: diccionarios de diccionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"rnn\": {},\n",
    "    \"lstm\": {},\n",
    "    \"gru\": {}\n",
    "}\n",
    "\n",
    "criterions = {\n",
    "    \"rnn\": {},\n",
    "    \"lstm\": {},\n",
    "    \"gru\": {}\n",
    "}\n",
    "\n",
    "optimizers = {\n",
    "    \"rnn\": {},\n",
    "    \"lstm\": {},\n",
    "    \"gru\": {}\n",
    "}\n",
    "\n",
    "schedulers = {\n",
    "    \"rnn\": {},\n",
    "    \"lstm\": {},\n",
    "    \"gru\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Definir una funci√≥n para inicializar los componentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_components(model_class, input_size, hidden_size, output_size, device, num_layers = 1, lr = 1e-3):\n",
    "    \"\"\"\n",
    "    Funci√≥n para instanciar los componentes: modelo, funci√≥n de p√©rdida, optimizer, learning rate scheduler\n",
    "    \"\"\"\n",
    "    model = model_class(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "    scheduler = StepLR(\n",
    "    optimizer,   # tu optimizador\n",
    "    step_size=10,  # reduce cada 10 epochs\n",
    "    gamma=0.05     # reduce a la mitad (lr = lr * gamma)\n",
    ")\n",
    "\n",
    "    return model, criterion, optimizer, scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Construir modelos, criterions, optimizers y schedulers por parking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 32\n",
    "NUM_LAYERS = 1\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid, dataset in train_datasets.items():\n",
    "\n",
    "    #n¬∫ de caracteristicas para predecir de salida = 1 (disponibilidad en instantes anteriores)\n",
    "    INPUT_SIZE = dataset[0][0].shape[-1]\n",
    "\n",
    "    #tama√±o de salida = 1 (predecir valor siguiente a partir de los 24 anteriores)\n",
    "    OUTPUT_SIZE = dataset[0][1].shape[-1]\n",
    "\n",
    "    #RNN\n",
    "    model, criterion, optimizer, scheduler = init_model_components(VanillaLSTM, \n",
    "                                                                   input_size = INPUT_SIZE, \n",
    "                                                                   hidden_size = HIDDEN_SIZE,\n",
    "                                                                   output_size = OUTPUT_SIZE,\n",
    "                                                                   num_layers = NUM_LAYERS,\n",
    "                                                                   device = DEVICE,\n",
    "                                                                   lr= LEARNING_RATE)\n",
    "    models[\"rnn\"][pid] = model\n",
    "    criterions[\"rnn\"][pid] = criterion\n",
    "    optimizers[\"rnn\"][pid] = optimizer\n",
    "    schedulers[\"rnn\"][pid] = scheduler\n",
    "\n",
    "    #LSTM\n",
    "    model, criterion, optimizer, scheduler = init_model_components(VanillaLSTM,\n",
    "                                                                   input_size= INPUT_SIZE,\n",
    "                                                                   hidden_size = HIDDEN_SIZE,\n",
    "                                                                   output_size = OUTPUT_SIZE,\n",
    "                                                                   num_layers = NUM_LAYERS,\n",
    "                                                                   device = DEVICE,\n",
    "                                                                   lr= LEARNING_RATE)\n",
    "    models[\"lstm\"][pid] = model\n",
    "    criterions[\"lstm\"][pid] = criterion\n",
    "    optimizers[\"lstm\"][pid] = optimizer\n",
    "    schedulers[\"lstm\"][pid] = scheduler\n",
    "\n",
    "    #GRU\n",
    "    model, criterion, optimizer, scheduler = init_model_components(VanillaGRU,\n",
    "                                                                   input_size= INPUT_SIZE,\n",
    "                                                                   hidden_size = HIDDEN_SIZE,\n",
    "                                                                   output_size = OUTPUT_SIZE,\n",
    "                                                                   num_layers = NUM_LAYERS,\n",
    "                                                                   device = DEVICE,\n",
    "                                                                   lr= LEARNING_RATE)\n",
    "    models[\"gru\"][pid] = model\n",
    "    criterions[\"gru\"][pid] = criterion\n",
    "    optimizers[\"gru\"][pid] = optimizer\n",
    "    schedulers[\"gru\"][pid] = scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rnn': {6: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  7: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  8: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  13: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  34: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  75: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  77: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  78: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )},\n",
       " 'lstm': {6: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  7: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  8: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  13: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  34: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  75: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  77: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  78: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )},\n",
       " 'gru': {6: VanillaGRU(\n",
       "    (gru): GRU(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  7: VanillaGRU(\n",
       "    (gru): GRU(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  8: VanillaGRU(\n",
       "    (gru): GRU(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  13: VanillaGRU(\n",
       "    (gru): GRU(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  34: VanillaGRU(\n",
       "    (gru): GRU(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  75: VanillaGRU(\n",
       "    (gru): GRU(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  77: VanillaGRU(\n",
       "    (gru): GRU(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  78: VanillaGRU(\n",
       "    (gru): GRU(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )}}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Crear callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Callback para detener el entrenamiento anticipadamente si la m√©trica de validaci√≥n\n",
    "    no mejora tras un n√∫mero de √©pocas determinado (patience).\n",
    "\n",
    "    Args:\n",
    "        patience (int): N√∫mero de √©pocas sin mejora antes de detener.\n",
    "        min_delta (float): Cambio m√≠nimo considerado como mejora.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.should_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        \"\"\"\n",
    "        Eval√∫a si se debe detener el entrenamiento.\n",
    "\n",
    "        Args:\n",
    "            val_loss (float): P√©rdida (loss) de validaci√≥n actual.\n",
    "\n",
    "        Returns:\n",
    "            bool: True si debe detenerse, False en caso contrario.\n",
    "        \"\"\"\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.should_stop = True\n",
    "        return self.should_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(model, val_loss, best_loss, path):\n",
    "    \"\"\"\n",
    "    Guarda el modelo si su loss de validaci√≥n mejora respecto al anterior.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo actual.\n",
    "        val_loss (float): Loss de validaci√≥n actual.\n",
    "        best_loss (float): Mejor loss registrado hasta el momento.\n",
    "        path (str): Ruta donde guardar el modelo si mejora.\n",
    "\n",
    "    Returns:\n",
    "        float: Nuevo mejor loss (actual o anterior si no mejor√≥).\n",
    "    \"\"\"\n",
    "    if val_loss < best_loss:\n",
    "        torch.save(model.state_dict(), path)\n",
    "        return val_loss  # se actualiza el best_loss\n",
    "    return best_loss  # se mantiene igual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Definir funci√≥n de validaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch(\n",
    "        model,\n",
    "        dataloader,\n",
    "        criterion,\n",
    "        metrics, \n",
    "        device \n",
    "):\n",
    "    \"\"\"\n",
    "    Ejecuta una pasada de validaci√≥n completa (1 epoch) sobre un dataloader.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo PyTorch a evaluar.\n",
    "        dataloader (DataLoader): Dataloader con los datos de validaci√≥n.\n",
    "        criterion (nn.Module): Funci√≥n de p√©rdida (loss).\n",
    "        metrics (torchmetrics.MetricCollection): M√©tricas a evaluar.\n",
    "        device (str): \"cpu\" o \"cuda\".\n",
    "\n",
    "    Returns:\n",
    "        dict: Diccionario con loss promedio y m√©tricas calculadas.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval() #poner modelo en modo evaluaci√≥n\n",
    "\n",
    "    #inicializar loss, n¬∫ batches y m√©tricas\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    metrics.to(device)\n",
    "    metrics.reset()\n",
    "\n",
    "    with torch.no_grad(): #desactivar c√°lculo de gradientes\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            # 1. Forward\n",
    "            y_pred = model(x_batch)\n",
    "\n",
    "            # 2. Calcular loss y a√±adirlo a lista de losses\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            running_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            #acumular valores del batch\n",
    "            metrics.update(y_pred, y_batch)\n",
    "    \n",
    "    #calcular loss promedio del batch\n",
    "    avg_loss = running_loss/num_batches\n",
    "\n",
    "    #calcular metrica total: promedio de metricas de cada batch\n",
    "    metric_results = metrics.compute()\n",
    "\n",
    "    #crear diccionario de resutaldos: primer elemento del diccionario es el loss\n",
    "    results = {\"val_loss\": avg_loss}\n",
    "\n",
    "    #a√±adir elementos al diccionario: cada par clave-valor es una m√©trica\n",
    "    results.update({k:v.item() for k,v in metric_results.items()})\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def validate_one_epoch(\n",
    "    model,\n",
    "    dataloader,\n",
    "    criterion,\n",
    "    metrics, \n",
    "    device,\n",
    "    epoch=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Ejecuta una pasada de validaci√≥n completa (1 epoch) sobre un dataloader.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo PyTorch a evaluar.\n",
    "        dataloader (DataLoader): Dataloader con los datos de validaci√≥n.\n",
    "        criterion (nn.Module): Funci√≥n de p√©rdida (loss).\n",
    "        metrics (torchmetrics.MetricCollection): M√©tricas a evaluar.\n",
    "        device (str): \"cpu\" o \"cuda\".\n",
    "        epoch (int, optional): N√∫mero de epoch actual (solo para mostrar).\n",
    "\n",
    "    Returns:\n",
    "        dict: Diccionario con loss promedio y m√©tricas calculadas.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()  # poner modelo en modo evaluaci√≥n\n",
    "\n",
    "    running_loss = 0.0\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    metrics.to(device)\n",
    "    metrics.reset()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # tqdm para barra de progreso\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch} [Val]\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in progress_bar:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            # Forward\n",
    "            y_pred = model(x_batch)\n",
    "\n",
    "            # Calcular loss y acumularlo\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Actualizar m√©tricas\n",
    "            metrics.update(y_pred, y_batch)\n",
    "\n",
    "            # Mostrar loss promedio hasta el batch actual\n",
    "            batch_idx = progress_bar.n\n",
    "            avg_loss_so_far = running_loss / (batch_idx + 1)\n",
    "            progress_bar.set_postfix({\"Loss\": avg_loss_so_far})\n",
    "\n",
    "    #calcular loss promedio del batch\n",
    "    avg_loss = running_loss / num_batches\n",
    "\n",
    "    #calcular metrica total: promedio de metricas de cada batch\n",
    "    metric_results = metrics.compute()\n",
    "\n",
    "    #crear diccionario de resutaldos: primer elemento del diccionario es el loss\n",
    "    results = {\"val_loss\": avg_loss}\n",
    "    #a√±adir elementos al diccionario: cada par clave-valor es una m√©trica\n",
    "    results.update({k: v.item() for k, v in metric_results.items()})\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch} - Val   | Loss: {avg_loss:.4f} | Time: {elapsed_time:.1f}s\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos que no hay inconsistencias, a pesar de que los pesos de los modelos sean aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Val   | Loss: 81.9369 | Time: 0.6s\n",
      "{'val_loss': 81.93691555658977, 'MAE': 7.372611999511719, 'MAPE': 28434.85546875, 'MSE': 82.05606079101562, 'RMSE': 9.058480262756348}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "val_results = validate_one_epoch(\n",
    "    model=models[\"lstm\"][6],\n",
    "    dataloader=val_dataloaders[6],\n",
    "    criterion=criterions[\"lstm\"][6],\n",
    "    metrics = metrics,\n",
    "    device= DEVICE,\n",
    "    epoch=1\n",
    ")\n",
    "\n",
    "print(val_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Definir bucle de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "                model,\n",
    "                dataloader,\n",
    "                criterion,\n",
    "                optimizer,\n",
    "                metrics,\n",
    "                device,\n",
    "                epoch = None\n",
    "):\n",
    "        \"\"\"\n",
    "        Ejecuta una epoch completa de entrenamiento.\n",
    "\n",
    "        Args:\n",
    "                model (nn.Module): Modelo a entrenar.\n",
    "                dataloader (DataLoader): Dataloader con datos de entrenamiento.\n",
    "                criterion (nn.Module): Funci√≥n de p√©rdida.\n",
    "                optimizer (Optimizer): Optimizador para actualizar pesos.\n",
    "                metrics (torchmetrics.MetricCollection): M√©tricas de evaluaci√≥n.\n",
    "                device (str): \"cpu\" o \"cuda\".\n",
    "                epoch (int, optional): N√∫mero de la √©poca actual (solo para mostrar).\n",
    "\n",
    "        Returns:\n",
    "                dict: Diccionario con loss promedio y m√©tricas acumuladas.\n",
    "        \"\"\"\n",
    "        \n",
    "        model.train() #poner modelo en modo entrenamiento\n",
    "\n",
    "        #inicializar loss y n¬∫ de batches\n",
    "        running_loss = 0.0\n",
    "        num_batches = len(dataloader)\n",
    "\n",
    "        #inicializar m√©tricas y moverlas al device\n",
    "        metrics.to(device)\n",
    "        metrics.reset()\n",
    "\n",
    "        #coger tiempo actual de referencia\n",
    "        start_time = time.time()\n",
    "\n",
    "        #inicializar barra de progreso\n",
    "        #progress_bar envuelve el dataloader y lo monitoriza\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch}\", leave=False)\n",
    "\n",
    "\n",
    "        for x_batch, y_batch in progress_bar:\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                optimizer.zero_grad() #1. resetear gradientes\n",
    "\n",
    "                y_pred = model(x_batch) #2. Forward\n",
    "\n",
    "                loss = criterion(y_pred, y_batch) # 3. calcular loss\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                batch_idx = progress_bar.n #indice del batch\n",
    "                avg_loss_so_far = running_loss / (batch_idx + 1)\n",
    "\n",
    "                #Mostrar en tiempo real el loss\n",
    "                progress_bar.set_postfix({\"Loss\": avg_loss_so_far})\n",
    "\n",
    "\n",
    "                loss.backward() #4. Propagar loss\n",
    "                optimizer.step() #5. Actualizar pesos\n",
    "\n",
    "\n",
    "                metrics.update(y_pred, y_batch)\n",
    "        \n",
    "        #calcular loss promedio del batch\n",
    "        avg_loss = running_loss/num_batches\n",
    "\n",
    "        #calcular metrica total: promedio de metricas de cada batch\n",
    "        metric_results = metrics.compute()\n",
    "\n",
    "        #crear diccionario de resutaldos: primer elemento del diccionario es el loss\n",
    "        results = {\"train_loss\": avg_loss}\n",
    "\n",
    "        #a√±adir elementos al diccionario: cada par clave-valor es una m√©trica\n",
    "        results.update({k: v.item() for k,v in metric_results.items()})\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Epoch {epoch} - Train | Loss: {avg_loss:.4f} | Time: {elapsed_time:.1f}s\")\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos que no hay inconsistencias, a pesar de que los pesos de los modelos sean aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train | Loss: 93.8941 | Time: 6.1s\n",
      "{'train_loss': 93.89411600924086, 'MAE': 7.749772548675537, 'MAPE': 3310.0859375, 'MSE': 93.90457153320312, 'RMSE': 9.690437316894531}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "train_results = train_one_epoch(\n",
    "    model=models[\"lstm\"][6],\n",
    "    dataloader=train_dataloaders[6],\n",
    "    criterion=criterions[\"lstm\"][6],\n",
    "    optimizer=optimizers[\"lstm\"][6],\n",
    "    metrics=metrics,\n",
    "    device=DEVICE,\n",
    "    epoch = 1\n",
    ")\n",
    "\n",
    "print(train_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Optimizaci√≥ √≥ptima de hiperpar√°metros con `Optuna`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    metrics,\n",
    "    scheduler=None,\n",
    "    num_epochs=10,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Entrena un modelo durante varias √©pocas y eval√∫a en validaci√≥n.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo a entrenar.\n",
    "        train_loader (DataLoader): Dataloader de entrenamiento.\n",
    "        val_loader (DataLoader): Dataloader de validaci√≥n.\n",
    "        criterion (nn.Module): Funci√≥n de p√©rdida.\n",
    "        optimizer (Optimizer): Optimizador.\n",
    "        metrics (torchmetrics.MetricCollection): M√©tricas compartidas entre train/val.\n",
    "        scheduler (optional): Scheduler de LR.\n",
    "        num_epochs (int): N√∫mero de √©pocas.\n",
    "        device (str): \"cpu\" o \"cuda\".\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de dicts con m√©tricas de cada √©poca.\n",
    "    \"\"\"\n",
    "\n",
    "    history = []  # Para guardar m√©tricas por √©poca\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "\n",
    "        # Entrenamiento\n",
    "        train_metrics = train_one_epoch(\n",
    "            model=model,\n",
    "            dataloader=train_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            metrics=metrics.clone(),  # ‚Üê importante: evita conflictos\n",
    "            device=device,\n",
    "            epoch=epoch\n",
    "        )\n",
    "\n",
    "        # Validaci√≥n\n",
    "        val_metrics = validate_one_epoch(\n",
    "            model=model,\n",
    "            dataloader=val_loader,\n",
    "            criterion=criterion,\n",
    "            metrics=metrics.clone(),\n",
    "            device=device,\n",
    "            epoch=epoch\n",
    "        )\n",
    "\n",
    "        # Scheduler (opcional)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Unir m√©tricas\n",
    "        combined = {\"epoch\": epoch}\n",
    "        combined.update(train_metrics)\n",
    "        combined.update(val_metrics)\n",
    "        history.append(combined)\n",
    "\n",
    "        # Mostrar resumen\n",
    "        print(\n",
    "            f\"üìä Summary | Train Loss: {train_metrics['train_loss']:.4f} | \"\n",
    "            f\"Val Loss: {val_metrics['val_loss']:.4f}\"\n",
    "        )\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train | Loss: 14.9539 | Time: 6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Val   | Loss: 16.3196 | Time: 0.8s\n",
      "üìä Summary | Train Loss: 14.9539 | Val Loss: 16.3196\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train | Loss: 14.9239 | Time: 6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Val   | Loss: 16.4341 | Time: 0.6s\n",
      "üìä Summary | Train Loss: 14.9239 | Val Loss: 16.4341\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train | Loss: 14.9418 | Time: 6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Val   | Loss: 16.3599 | Time: 0.6s\n",
      "üìä Summary | Train Loss: 14.9418 | Val Loss: 16.3599\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train | Loss: 14.9173 | Time: 6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Val   | Loss: 16.4595 | Time: 0.6s\n",
      "üìä Summary | Train Loss: 14.9173 | Val Loss: 16.4595\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train | Loss: 14.9104 | Time: 6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Val   | Loss: 16.3092 | Time: 0.6s\n",
      "üìä Summary | Train Loss: 14.9104 | Val Loss: 16.3092\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train | Loss: 14.9152 | Time: 5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Val   | Loss: 16.3922 | Time: 0.6s\n",
      "üìä Summary | Train Loss: 14.9152 | Val Loss: 16.3922\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train | Loss: 14.9080 | Time: 5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Val   | Loss: 16.3527 | Time: 0.6s\n",
      "üìä Summary | Train Loss: 14.9080 | Val Loss: 16.3527\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train | Loss: 14.9048 | Time: 6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Val   | Loss: 16.2733 | Time: 0.6s\n",
      "üìä Summary | Train Loss: 14.9048 | Val Loss: 16.2733\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train | Loss: 14.9064 | Time: 5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Val   | Loss: 16.4183 | Time: 0.6s\n",
      "üìä Summary | Train Loss: 14.9064 | Val Loss: 16.4183\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train | Loss: 14.9069 | Time: 6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Val   | Loss: 16.3308 | Time: 0.6s\n",
      "üìä Summary | Train Loss: 14.9069 | Val Loss: 16.3308\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Train | Loss: 14.8986 | Time: 6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Val   | Loss: 16.2677 | Time: 0.6s\n",
      "üìä Summary | Train Loss: 14.8986 | Val Loss: 16.2677\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Train | Loss: 14.8944 | Time: 6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Val   | Loss: 16.2855 | Time: 0.6s\n",
      "üìä Summary | Train Loss: 14.8944 | Val Loss: 16.2855\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Train | Loss: 14.8972 | Time: 6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Val   | Loss: 16.2984 | Time: 0.6s\n",
      "üìä Summary | Train Loss: 14.8972 | Val Loss: 16.2984\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Train | Loss: 14.8934 | Time: 6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Val   | Loss: 16.2335 | Time: 0.6s\n",
      "üìä Summary | Train Loss: 14.8934 | Val Loss: 16.2335\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Train | Loss: 14.9026 | Time: 6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Val   | Loss: 16.2823 | Time: 0.8s\n",
      "üìä Summary | Train Loss: 14.9026 | Val Loss: 16.2823\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Train | Loss: 14.8924 | Time: 6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Val   | Loss: 16.2687 | Time: 0.6s\n",
      "üìä Summary | Train Loss: 14.8924 | Val Loss: 16.2687\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Train | Loss: 14.8951 | Time: 6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Val   | Loss: 16.2463 | Time: 0.6s\n",
      "üìä Summary | Train Loss: 14.8951 | Val Loss: 16.2463\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Train | Loss: 14.8930 | Time: 6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Val   | Loss: 16.2183 | Time: 0.6s\n",
      "üìä Summary | Train Loss: 14.8930 | Val Loss: 16.2183\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Train | Loss: 14.8904 | Time: 5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Val   | Loss: 16.2984 | Time: 0.6s\n",
      "üìä Summary | Train Loss: 14.8904 | Val Loss: 16.2984\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Train | Loss: 14.9233 | Time: 5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Val   | Loss: 16.2800 | Time: 0.6s\n",
      "üìä Summary | Train Loss: 14.9233 | Val Loss: 16.2800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "history = fit(\n",
    "    model=models[\"lstm\"][6],\n",
    "    train_loader=train_dataloaders[6],\n",
    "    val_loader=val_dataloaders[6],\n",
    "    criterion=criterions[\"lstm\"][6],\n",
    "    optimizer=optimizers[\"lstm\"][6],\n",
    "    scheduler=schedulers[\"lstm\"][6],\n",
    "    metrics=metrics,  # una instancia de MetricCollection\n",
    "    num_epochs=20,\n",
    "    device=DEVICE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Predicciones sobre el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Obtiene las predicciones del modelo sobre un dataloader completo.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo entrenado.\n",
    "        dataloader (DataLoader): Dataloader a predecir (train, val o test).\n",
    "        device (str): \"cpu\" o \"cuda\".\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor]: (y_true, y_pred), ambos de tama√±o (N, output_size)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            y_pred = model(x_batch)\n",
    "\n",
    "            all_preds.append(y_pred.cpu())\n",
    "            all_targets.append(y_batch.cpu())\n",
    "\n",
    "    y_pred = torch.cat(all_preds, dim=0)\n",
    "    y_true = torch.cat(all_targets, dim=0)\n",
    "\n",
    "    return y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2758, 1]) torch.Size([2758, 1])\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = predict(\n",
    "    model=models[\"lstm\"][6],\n",
    "    dataloader=test_dataloaders[6],\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "print(y_true.shape, y_pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[62.9032],\n",
       "        [59.1398],\n",
       "        [59.4086],\n",
       "        [59.4086],\n",
       "        [59.1398]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[67.1369],\n",
       "        [53.9262],\n",
       "        [55.5845],\n",
       "        [59.7052],\n",
       "        [58.4262]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Exportaci√≥n de checkpoints y logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TS-PARKINGS_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
