{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelado de Series Temporales de Aparcamientos usando RNNs y optimización con Optuna\n",
    "En este notebook se abordará un problema de predicción de series temporales utilizando datos de disponibilidad de parkings.\n",
    "\n",
    "Flujo de trabajo:\n",
    "1. Agrupación de los datos por id de aparcamiento (idAparcamiento), para tratar cada parking como una serie temporal independiente.\n",
    "2. División del conjunto de datos en tres subconjuntos: entrenamiento (train), validación (val) y prueba (test).\n",
    "3. Entrenamiento de modelos de redes neuronales recurrentes simples (vanilla):\n",
    " - Vanilla RNN\n",
    " - Vanilla GRU\n",
    " - Vanilla LSTM\n",
    "4. Ajuste de hiperparámetros utilizando Optuna para encontrar la configuración óptima en cada tipo de modelo.\n",
    "5. Comparación de los resultados de rendimiento entre los distintos modelos utilizando métricas apropiadas.\n",
    "\n",
    "El objetivo final es evaluar qué arquitectura ofrece mejores resultados para este tipo de datos y tarea de predicción.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import torchmetrics\n",
    "from torchmetrics import MetricCollection\n",
    "from torchmetrics.regression import MeanAbsoluteError, MeanSquaredError, MeanAbsolutePercentageError, R2Score\n",
    "\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "HIDDEN_SIZE = 32\n",
    "NUM_LAYERS = 1\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idAparcamiento</th>\n",
       "      <th>PlazasTotales</th>\n",
       "      <th>PlazasDisponibles</th>\n",
       "      <th>PorcPlazasDisponibles</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>weekday</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-02-03 10:00:00</th>\n",
       "      <td>6</td>\n",
       "      <td>372.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>16.129032</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-03 11:00:00</th>\n",
       "      <td>6</td>\n",
       "      <td>372.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>12.903226</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-03 12:00:00</th>\n",
       "      <td>6</td>\n",
       "      <td>372.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>17.741935</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-03 13:00:00</th>\n",
       "      <td>6</td>\n",
       "      <td>372.0</td>\n",
       "      <td>119.0</td>\n",
       "      <td>31.989247</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-02-03 14:00:00</th>\n",
       "      <td>6</td>\n",
       "      <td>372.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>41.666667</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-05 03:00:00</th>\n",
       "      <td>78</td>\n",
       "      <td>464.0</td>\n",
       "      <td>355.0</td>\n",
       "      <td>76.508621</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-05 04:00:00</th>\n",
       "      <td>78</td>\n",
       "      <td>464.0</td>\n",
       "      <td>355.0</td>\n",
       "      <td>76.508621</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-05 05:00:00</th>\n",
       "      <td>78</td>\n",
       "      <td>464.0</td>\n",
       "      <td>356.0</td>\n",
       "      <td>76.724138</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-05 06:00:00</th>\n",
       "      <td>78</td>\n",
       "      <td>464.0</td>\n",
       "      <td>354.0</td>\n",
       "      <td>76.293103</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025-03-05 07:00:00</th>\n",
       "      <td>78</td>\n",
       "      <td>464.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>71.551724</td>\n",
       "      <td>2025</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149395 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     idAparcamiento  PlazasTotales  PlazasDisponibles  \\\n",
       "timestamp                                                               \n",
       "2023-02-03 10:00:00               6          372.0               60.0   \n",
       "2023-02-03 11:00:00               6          372.0               48.0   \n",
       "2023-02-03 12:00:00               6          372.0               66.0   \n",
       "2023-02-03 13:00:00               6          372.0              119.0   \n",
       "2023-02-03 14:00:00               6          372.0              155.0   \n",
       "...                             ...            ...                ...   \n",
       "2025-03-05 03:00:00              78          464.0              355.0   \n",
       "2025-03-05 04:00:00              78          464.0              355.0   \n",
       "2025-03-05 05:00:00              78          464.0              356.0   \n",
       "2025-03-05 06:00:00              78          464.0              354.0   \n",
       "2025-03-05 07:00:00              78          464.0              332.0   \n",
       "\n",
       "                     PorcPlazasDisponibles  year  month  day  weekday  \n",
       "timestamp                                                              \n",
       "2023-02-03 10:00:00              16.129032  2023      2    3        4  \n",
       "2023-02-03 11:00:00              12.903226  2023      2    3        4  \n",
       "2023-02-03 12:00:00              17.741935  2023      2    3        4  \n",
       "2023-02-03 13:00:00              31.989247  2023      2    3        4  \n",
       "2023-02-03 14:00:00              41.666667  2023      2    3        4  \n",
       "...                                    ...   ...    ...  ...      ...  \n",
       "2025-03-05 03:00:00              76.508621  2025      3    5        2  \n",
       "2025-03-05 04:00:00              76.508621  2025      3    5        2  \n",
       "2025-03-05 05:00:00              76.724138  2025      3    5        2  \n",
       "2025-03-05 06:00:00              76.293103  2025      3    5        2  \n",
       "2025-03-05 07:00:00              71.551724  2025      3    5        2  \n",
       "\n",
       "[149395 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/processed/data_processed.csv\")\n",
    "\n",
    "#convertir a indice\n",
    "df.set_index(\"timestamp\", inplace= True)\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. División del conjunto de datos\n",
    "\n",
    "Realizamos la división del conjunto de datos, agrupando por `idAparcamiento`. \n",
    "La idea es que el conjunto de **test sea común** para todos los aparcamientos, correspondiente al **último 10% del rango temporal total** del dataset. \n",
    "\n",
    "El resto de los datos disponibles para cada parking se dividen en:\n",
    "\n",
    "- **Entrenamiento (train)**: el primer 85% de los datos previos al test.\n",
    "- **Validación (val)**: el último 15% de los datos previos al test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "# 1. Calcular el rango temporal global\n",
    "fecha_min_global = df.index.min()\n",
    "fecha_max_global = df.index.max()\n",
    "rango_total = fecha_max_global - fecha_min_global\n",
    "\n",
    "# 2. Calcular el inicio del conjunto de test (último 10% del rango)\n",
    "test_ratio = 0.10\n",
    "test_duration = timedelta(seconds=rango_total.total_seconds() * test_ratio)\n",
    "test_start = fecha_max_global - test_duration\n",
    "\n",
    "# 3. Diccionarios para almacenar los splits\n",
    "train_dict = {}\n",
    "val_dict = {}\n",
    "test_dict = {}\n",
    "\n",
    "val_ratio = 0.1  # del conjunto anterior al test\n",
    "\n",
    "# 4. División por parking\n",
    "for parking_id, group in df.groupby(\"idAparcamiento\"):\n",
    "    group = group.sort_index()\n",
    "    \n",
    "    # Split basado en el corte global\n",
    "    test_set = group[group.index >= test_start]\n",
    "    remaining = group[group.index < test_start]\n",
    "\n",
    "    # Dividir en train y val\n",
    "    val_size = int(len(remaining) * val_ratio)\n",
    "    val_set = remaining.iloc[-val_size:]\n",
    "    train_set = remaining.iloc[:-val_size]\n",
    "    \n",
    "    # Guardar resultados\n",
    "    train_dict[parking_id] = train_set\n",
    "    val_dict[parking_id] = val_set\n",
    "    test_dict[parking_id] = test_set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Definir `Dataset` de pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, input_window, output_window, feature_cols, target_col):\n",
    "        \"\"\"\n",
    "        data: DataFrame que contiene los datos de la serie temporal\n",
    "        input_window: nº de pasos de tiempo en la secuencia de entrada\n",
    "        output_window: nº de pasos de tiempo a predecir\n",
    "        feature_cols: lista de nombres de columnas que se usan como característcas\n",
    "        target_col: nombre de la variable a predecir\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.input_window = input_window\n",
    "        self.output_window = output_window\n",
    "        self.feature_cols = feature_cols\n",
    "        self.target_cols = target_col\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Función que devuele el nº de datos del Dataset\n",
    "        \"\"\"\n",
    "        return len(self.data) - self.input_window - self.output_window + 1 #\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Función que devuelve un dato a partir de un índice\n",
    "        \"\"\"\n",
    "        X = self.data[idx: idx + self.input_window][self.feature_cols].values\n",
    "        Y = self.data[idx + self.input_window: idx + self.input_window + self.output_window][self.target_cols].values\n",
    "        \n",
    "        X_tensor = torch.tensor(X, dtype= torch.float32)\n",
    "        Y_tensor = torch.tensor(Y, dtype= torch.float32)\n",
    "\n",
    "        return X_tensor, Y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['PorcPlazasDisponibles']  \n",
    "target_col = 'PorcPlazasDisponibles'     \n",
    "input_window = 24         # Número de pasos de tiempo en la secuencia de entrada\n",
    "output_window = 1         # Número de pasos de tiempo a predecir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que tenemos una serie temporal por parking, construimos un diccionario, donde las claves son los identificadores de parkings y los valores son Daatasets de tipo `torch.utils.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = {\n",
    "    pid: TimeSeriesDataset(df, input_window, output_window, feature_cols, target_col=target_col)\n",
    "    for pid, df in train_dict.items()\n",
    "}\n",
    "val_datasets = {\n",
    "    pid: TimeSeriesDataset(df, input_window, output_window, feature_cols, target_col=target_col)\n",
    "    for pid, df in val_dict.items()\n",
    "}\n",
    "test_datasets = {\n",
    "    pid: TimeSeriesDataset(df, input_window, output_window, feature_cols, target_col=target_col)\n",
    "    for pid, df in test_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id parking:  6\n",
      "longitud:  13908\n",
      "id parking:  7\n",
      "longitud:  13908\n",
      "id parking:  8\n",
      "longitud:  22507\n",
      "id parking:  13\n",
      "longitud:  13908\n",
      "id parking:  34\n",
      "longitud:  9310\n",
      "id parking:  75\n",
      "longitud:  22505\n",
      "id parking:  77\n",
      "longitud:  10735\n",
      "id parking:  78\n",
      "longitud:  7454\n"
     ]
    }
   ],
   "source": [
    "for pid, df in train_datasets.items():\n",
    "    print(\"id parking: \", pid)\n",
    "    print(\"longitud: \", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Crear `DataLoaders` a partir de `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear los `DataLoader`, seguimos el mismo criterio, es decir, crear un diccionario de DataLoaders, en el que cada iteración nos devuelve un batch de datos para cada parking.\n",
    "- Cada batch de datos debe tener dimensión: `(batch_size, window_size, n_features)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloaders = {\n",
    "    pid: DataLoader(ts_dataset, batch_size = 32, shuffle = True) #(batch_size, window_size, n_features)\n",
    "    for pid, ts_dataset in train_datasets.items()\n",
    "}\n",
    "\n",
    "val_dataloaders = {\n",
    "    pid: DataLoader(ts_dataset, batch_size = 32, shuffle = True) #(batch_size, window_size, n_features)\n",
    "    for pid, ts_dataset in val_datasets.items()\n",
    "}\n",
    "\n",
    "test_dataloaders = {\n",
    "    pid: DataLoader(ts_dataset, batch_size = 64) #(batch_size, window_size, n_features)\n",
    "    for pid, ts_dataset in test_datasets.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id parking 6\n",
      "Dimensión del primer batch de datos: torch.Size([32, 24, 1])\n",
      "Dimensión del primer batch de etiquetas:  torch.Size([32, 1])\n",
      "\n",
      "\n",
      "id parking 7\n",
      "Dimensión del primer batch de datos: torch.Size([32, 24, 1])\n",
      "Dimensión del primer batch de etiquetas:  torch.Size([32, 1])\n",
      "\n",
      "\n",
      "id parking 8\n",
      "Dimensión del primer batch de datos: torch.Size([32, 24, 1])\n",
      "Dimensión del primer batch de etiquetas:  torch.Size([32, 1])\n",
      "\n",
      "\n",
      "id parking 13\n",
      "Dimensión del primer batch de datos: torch.Size([32, 24, 1])\n",
      "Dimensión del primer batch de etiquetas:  torch.Size([32, 1])\n",
      "\n",
      "\n",
      "id parking 34\n",
      "Dimensión del primer batch de datos: torch.Size([32, 24, 1])\n",
      "Dimensión del primer batch de etiquetas:  torch.Size([32, 1])\n",
      "\n",
      "\n",
      "id parking 75\n",
      "Dimensión del primer batch de datos: torch.Size([32, 24, 1])\n",
      "Dimensión del primer batch de etiquetas:  torch.Size([32, 1])\n",
      "\n",
      "\n",
      "id parking 77\n",
      "Dimensión del primer batch de datos: torch.Size([32, 24, 1])\n",
      "Dimensión del primer batch de etiquetas:  torch.Size([32, 1])\n",
      "\n",
      "\n",
      "id parking 78\n",
      "Dimensión del primer batch de datos: torch.Size([32, 24, 1])\n",
      "Dimensión del primer batch de etiquetas:  torch.Size([32, 1])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for pid, dloader in train_dataloaders.items():\n",
    "    print(\"id parking\", pid)\n",
    "    for batch in dloader:\n",
    "        print(\"Dimensión del primer batch de datos:\", batch[0].shape)\n",
    "        print(\"Dimensión del primer batch de etiquetas: \", batch[1].shape)\n",
    "        break\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Definir Modelos (`RNN`,`GRU`,`LSTM`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Clase que implementa una RNN vanilla: 1 capa, 1 neurona por capa oculta\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.rnn = nn.RNN(input_size = self.input_size,\n",
    "                          hidden_size = self.hidden_size,\n",
    "                          num_layers = self.num_layers,\n",
    "                          batch_first = True)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "    \n",
    "    def forward(self, x, h0 = None):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size_ = x.size(0)\n",
    "        #si no se pasa el primer estado oculto, lo inicializamos con 0\n",
    "        if h0 is None:\n",
    "            h0 = torch.zeros(self.num_layers, batch_size_, self.hidden_size).to(x.device)\n",
    "        \n",
    "        #all_hidden_states (output) --> todos los estados ocultos (batch_size, seq_length, hidden_size)\n",
    "        #last_hidden_state (h_n) --> ultimo estado oculto (num_layers, batch_size, hidden_size)\n",
    "        all_hidden_states, last_hidden_state = self.rnn(x, h0)\n",
    "\n",
    "        last_hidde_state_of_last_layer = last_hidden_state[-1]\n",
    "        \n",
    "        #si no pasamos por una capa densa, el modelo no hace la prediccion\n",
    "        pred = self.fc(last_hidde_state_of_last_layer)  # (batch_size, output_size)\n",
    "\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que no hay inconsistencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "model = VanillaRNN(input_size=1, hidden_size=64, output_size=1, num_layers=1)\n",
    "x = torch.randn(32, 24, 1)  # (batch_size, sequence_length, num_features)\n",
    "y  = model(x)\n",
    "print(y.shape)  # → (32, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VanillaLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x, h0=None, c0=None):\n",
    "        batch_size_ = x.size(0)\n",
    "        #si no se pasa el primer h0 o c0, lo inicializamos con 0\n",
    "        if h0 is None:\n",
    "            h0 = torch.zeros(self.num_layers, batch_size_, self.hidden_size).to(x.device)\n",
    "        if c0 is None:\n",
    "            c0 = torch.zeros(self.num_layers, batch_size_, self.hidden_size).to(x.device)\n",
    "\n",
    "        all_hidden_states, (last_hidden_state, last_cell_memory) = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Usar el último estado oculto de la última capa\n",
    "        pred = self.fc(last_hidden_state[-1])  # Shape: (batch_size, output_size)\n",
    "        \n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos que no hay inconsistencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "model = VanillaLSTM(input_size=1, hidden_size=64, output_size=1, num_layers=1)\n",
    "x = torch.randn(32, 24, 1)  # (batch_size, sequence_length, num_features)\n",
    "y  = model(x)\n",
    "print(y.shape)  #(32, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers = 1):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.gru = nn.GRU(input_size = self.input_size,\n",
    "                          hidden_size = self.hidden_size,\n",
    "                          num_layers = self.num_layers,\n",
    "                          batch_first= True)\n",
    "        \n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "    \n",
    "    def forward(self, x, h0=None):\n",
    "        \n",
    "        batch_size_ = x.size(0)\n",
    "        \n",
    "        ##si no se pasa el primer h0, lo inicializamos con 0\n",
    "        if h0 is None:\n",
    "            h0 = torch.zeros(self.num_layers, batch_size_, self.hidden_size).to(x.device)\n",
    "        \n",
    "        #all_hidden_states (output) --> todos los estados ocultos (batch_size, seq_length, hidden_size)\n",
    "        #last_hidden_state (h_n) --> ultimo estado oculto (num_layers, batch_size, hidden_size)\n",
    "        all_hidden_states, last_hidden_state = self.gru(x, h0)\n",
    "\n",
    "        last_hidde_state_of_last_layer = last_hidden_state[-1]\n",
    "        \n",
    "        #si no pasamos por una capa densa, el modelo no hace la prediccion\n",
    "        pred = self.fc(last_hidde_state_of_last_layer)  # (batch_size, output_size)\n",
    "\n",
    "        return pred  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n"
     ]
    }
   ],
   "source": [
    "model = VanillaGRU(input_size=1, hidden_size=64, output_size=1, num_layers=1)\n",
    "x = torch.randn(32, 24, 1)  # (batch_size, sequence_length, num_features)\n",
    "y  = model(x)\n",
    "print(y.shape)  #(32, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Definir métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MetricCollection({\n",
    "    \"MAE\": MeanAbsoluteError(),\n",
    "    \"MSE\": MeanSquaredError(),\n",
    "    \"RMSE\": MeanSquaredError(squared = False),\n",
    "    \"MAPE\": MeanAbsolutePercentageError()\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Instanciar modelos, optimizador, función de coste y learning rate scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 💡 Estructura de entrenamiento modular por parking y arquitectura\n",
    "\n",
    "Dado que tenemos:\n",
    "\n",
    "- Un diccionario de datasets por `idAparcamiento`\n",
    "- Tres arquitecturas distintas: `RNN`, `LSTM`, y `GRU`\n",
    "\n",
    "Cada modelo se entrena **por separado para cada parking**, permitiendo una evaluación más precisa por serie temporal individual.\n",
    "\n",
    "Para hacerlo **flexible y escalable**, organizamos todos los componentes en **diccionarios anidados**. Cada uno almacena los elementos necesarios para entrenar y validar por `idAparcamiento` y por tipo de modelo:\n",
    "\n",
    "- `models[\"rnn\"][pid]`, `models[\"lstm\"][pid]`, etc.\n",
    "- `optimizers[\"gru\"][pid]`, `criterions[\"lstm\"][pid]`, etc.\n",
    "- `schedulers[\"rnn\"][pid]` para aplicar estrategias de LR por modelo\n",
    "\n",
    "Esto nos permite:\n",
    "\n",
    "- Entrenar múltiples arquitecturas de forma aislada y comparable  \n",
    "- Hacer tuning o validación cruzada por parking  \n",
    "- Comparar vanilla vs. modelos optimizados con Optuna  \n",
    "- Guardar y cargar modelos individuales por ID\n",
    "\n",
    "Esta estructura es especialmente útil en contextos donde las series tienen longitudes o comportamientos distintos, como ocurre con los parkings reales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_output_size(dataloader):\n",
    "    \"\"\"\n",
    "    Extrae el número de features de entrada y el tamaño de salida\n",
    "    desde un batch del DataLoader.\n",
    "    \n",
    "    Retorna:\n",
    "        input_size: int (número de features por paso temporal)\n",
    "        output_size: int (dimensión de la predicción por muestra)\n",
    "    \"\"\"\n",
    "    # Tomamos el primer batch del dataloader\n",
    "    batch = next(iter(dataloader))\n",
    "    x, y = batch\n",
    "    input_size = x.shape[-1]   # última dimensión de x --> n_features\n",
    "    output_size = y.shape[-1]  # última dimensión de y --> n_outputs\n",
    "    return input_size, output_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size (n_features): 1\n",
      "Output size: 1\n"
     ]
    }
   ],
   "source": [
    "dloader = train_dataloaders[6]\n",
    "INPUT_SIZE, OUTPUT_SIZE = get_input_output_size(dloader)\n",
    "\n",
    "print(\"Input size (n_features):\", INPUT_SIZE)\n",
    "print(\"Output size:\", OUTPUT_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Definir estructuras adecuadas: diccionarios de diccionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"rnn\": {},\n",
    "    \"lstm\": {},\n",
    "    \"gru\": {}\n",
    "}\n",
    "\n",
    "criterions = {\n",
    "    \"rnn\": {},\n",
    "    \"lstm\": {},\n",
    "    \"gru\": {}\n",
    "}\n",
    "\n",
    "optimizers = {\n",
    "    \"rnn\": {},\n",
    "    \"lstm\": {},\n",
    "    \"gru\": {}\n",
    "}\n",
    "\n",
    "schedulers = {\n",
    "    \"rnn\": {},\n",
    "    \"lstm\": {},\n",
    "    \"gru\": {}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Definir una función para inicializar los componentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_components(model_class, input_size, hidden_size, output_size, device, num_layers = 1, lr = 1e-3):\n",
    "    \"\"\"\n",
    "    Función para instanciar los componentes: modelo, función de pérdida, optimizer, learning rate scheduler\n",
    "    \"\"\"\n",
    "    model = model_class(input_size, hidden_size, output_size, num_layers).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "    scheduler = StepLR(\n",
    "    optimizer,   # tu optimizador\n",
    "    step_size=10,  # reduce cada 10 epochs\n",
    "    gamma=0.05     # reduce a la mitad (lr = lr * gamma)\n",
    ")\n",
    "\n",
    "    return model, criterion, optimizer, scheduler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Construir modelos, criterions, optimizers y schedulers por parking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 32\n",
    "NUM_LAYERS = 1\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pid, dataset in train_datasets.items():\n",
    "\n",
    "    #nº de caracteristicas para predecir de salida = 1 (disponibilidad en instantes anteriores)\n",
    "    INPUT_SIZE = dataset[0][0].shape[-1]\n",
    "\n",
    "    #tamaño de salida = 1 (predecir valor siguiente a partir de los 24 anteriores)\n",
    "    OUTPUT_SIZE = dataset[0][1].shape[-1]\n",
    "\n",
    "    #RNN\n",
    "    model, criterion, optimizer, scheduler = init_model_components(VanillaLSTM, \n",
    "                                                                   input_size = INPUT_SIZE, \n",
    "                                                                   hidden_size = HIDDEN_SIZE,\n",
    "                                                                   output_size = OUTPUT_SIZE,\n",
    "                                                                   num_layers = NUM_LAYERS,\n",
    "                                                                   device = DEVICE,\n",
    "                                                                   lr= LEARNING_RATE)\n",
    "    models[\"rnn\"][pid] = model\n",
    "    criterions[\"rnn\"][pid] = criterion\n",
    "    optimizers[\"rnn\"][pid] = optimizer\n",
    "    schedulers[\"rnn\"][pid] = scheduler\n",
    "\n",
    "    #LSTM\n",
    "    model, criterion, optimizer, scheduler = init_model_components(VanillaLSTM,\n",
    "                                                                   input_size= INPUT_SIZE,\n",
    "                                                                   hidden_size = HIDDEN_SIZE,\n",
    "                                                                   output_size = OUTPUT_SIZE,\n",
    "                                                                   num_layers = NUM_LAYERS,\n",
    "                                                                   device = DEVICE,\n",
    "                                                                   lr= LEARNING_RATE)\n",
    "    models[\"lstm\"][pid] = model\n",
    "    criterions[\"lstm\"][pid] = criterion\n",
    "    optimizers[\"lstm\"][pid] = optimizer\n",
    "    schedulers[\"lstm\"][pid] = scheduler\n",
    "\n",
    "    #GRU\n",
    "    model, criterion, optimizer, scheduler = init_model_components(VanillaGRU,\n",
    "                                                                   input_size= INPUT_SIZE,\n",
    "                                                                   hidden_size = HIDDEN_SIZE,\n",
    "                                                                   output_size = OUTPUT_SIZE,\n",
    "                                                                   num_layers = NUM_LAYERS,\n",
    "                                                                   device = DEVICE,\n",
    "                                                                   lr= LEARNING_RATE)\n",
    "    models[\"gru\"][pid] = model\n",
    "    criterions[\"gru\"][pid] = criterion\n",
    "    optimizers[\"gru\"][pid] = optimizer\n",
    "    schedulers[\"gru\"][pid] = scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rnn': {6: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  7: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  8: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  13: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  34: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  75: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  77: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  78: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )},\n",
       " 'lstm': {6: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  7: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  8: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  13: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  34: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  75: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  77: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  78: VanillaLSTM(\n",
       "    (lstm): LSTM(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )},\n",
       " 'gru': {6: VanillaGRU(\n",
       "    (gru): GRU(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  7: VanillaGRU(\n",
       "    (gru): GRU(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  8: VanillaGRU(\n",
       "    (gru): GRU(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  13: VanillaGRU(\n",
       "    (gru): GRU(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  34: VanillaGRU(\n",
       "    (gru): GRU(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  75: VanillaGRU(\n",
       "    (gru): GRU(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  77: VanillaGRU(\n",
       "    (gru): GRU(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  ),\n",
       "  78: VanillaGRU(\n",
       "    (gru): GRU(1, 32, batch_first=True)\n",
       "    (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )}}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Crear callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Callback para detener el entrenamiento anticipadamente si la métrica de validación\n",
    "    no mejora tras un número de épocas determinado (patience).\n",
    "\n",
    "    Args:\n",
    "        patience (int): Número de épocas sin mejora antes de detener.\n",
    "        min_delta (float): Cambio mínimo considerado como mejora.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.should_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        \"\"\"\n",
    "        Evalúa si se debe detener el entrenamiento.\n",
    "\n",
    "        Args:\n",
    "            val_loss (float): Pérdida (loss) de validación actual.\n",
    "\n",
    "        Returns:\n",
    "            bool: True si debe detenerse, False en caso contrario.\n",
    "        \"\"\"\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.should_stop = True\n",
    "        return self.should_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_best_model(model, val_loss, best_loss, path):\n",
    "    \"\"\"\n",
    "    Guarda el modelo si su loss de validación mejora respecto al anterior.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo actual.\n",
    "        val_loss (float): Loss de validación actual.\n",
    "        best_loss (float): Mejor loss registrado hasta el momento.\n",
    "        path (str): Ruta donde guardar el modelo si mejora.\n",
    "\n",
    "    Returns:\n",
    "        float: Nuevo mejor loss (actual o anterior si no mejoró).\n",
    "    \"\"\"\n",
    "    if val_loss < best_loss:\n",
    "        torch.save(model.state_dict(), path)\n",
    "        return val_loss  # se actualiza el best_loss\n",
    "    return best_loss  # se mantiene igual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Definir función de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_one_epoch(\n",
    "        model,\n",
    "        dataloader,\n",
    "        criterion,\n",
    "        metrics, \n",
    "        device \n",
    "):\n",
    "    \"\"\"\n",
    "    Ejecuta una pasada de validación completa (1 epoch) sobre un dataloader.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo PyTorch a evaluar.\n",
    "        dataloader (DataLoader): Dataloader con los datos de validación.\n",
    "        criterion (nn.Module): Función de pérdida (loss).\n",
    "        metrics (torchmetrics.MetricCollection): Métricas a evaluar.\n",
    "        device (str): \"cpu\" o \"cuda\".\n",
    "\n",
    "    Returns:\n",
    "        dict: Diccionario con loss promedio y métricas calculadas.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval() #poner modelo en modo evaluación\n",
    "\n",
    "    #inicializar loss, nº batches y métricas\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    metrics.to(device)\n",
    "    metrics.reset()\n",
    "\n",
    "    with torch.no_grad(): #desactivar cálculo de gradientes\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            # 1. Forward\n",
    "            y_pred = model(x_batch)\n",
    "\n",
    "            # 2. Calcular loss y añadirlo a lista de losses\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            running_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "            #acumular valores del batch\n",
    "            metrics.update(y_pred, y_batch)\n",
    "    \n",
    "    #calcular loss promedio del batch\n",
    "    avg_loss = running_loss/num_batches\n",
    "\n",
    "    #calcular metrica total: promedio de metricas de cada batch\n",
    "    metric_results = metrics.compute()\n",
    "\n",
    "    #crear diccionario de resutaldos: primer elemento del diccionario es el loss\n",
    "    results = {\"val_loss\": avg_loss}\n",
    "\n",
    "    #añadir elementos al diccionario: cada par clave-valor es una métrica\n",
    "    results.update({k:v.item() for k,v in metric_results.items()})\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def validate_one_epoch(\n",
    "    model,\n",
    "    dataloader,\n",
    "    criterion,\n",
    "    metrics, \n",
    "    device,\n",
    "    epoch=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Ejecuta una pasada de validación completa (1 epoch) sobre un dataloader.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo PyTorch a evaluar.\n",
    "        dataloader (DataLoader): Dataloader con los datos de validación.\n",
    "        criterion (nn.Module): Función de pérdida (loss).\n",
    "        metrics (torchmetrics.MetricCollection): Métricas a evaluar.\n",
    "        device (str): \"cpu\" o \"cuda\".\n",
    "        epoch (int, optional): Número de epoch actual (solo para mostrar).\n",
    "\n",
    "    Returns:\n",
    "        dict: Diccionario con loss promedio y métricas calculadas.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()  # poner modelo en modo evaluación\n",
    "\n",
    "    running_loss = 0.0\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    metrics.to(device)\n",
    "    metrics.reset()\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # tqdm para barra de progreso\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch} [Val]\", leave=False)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in progress_bar:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            # Forward\n",
    "            y_pred = model(x_batch)\n",
    "\n",
    "            # Calcular loss y acumularlo\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Actualizar métricas\n",
    "            metrics.update(y_pred, y_batch)\n",
    "\n",
    "            # Mostrar loss promedio hasta el batch actual\n",
    "            batch_idx = progress_bar.n\n",
    "            avg_loss_so_far = running_loss / (batch_idx + 1)\n",
    "            progress_bar.set_postfix({\"Loss\": avg_loss_so_far})\n",
    "\n",
    "    #calcular loss promedio del batch\n",
    "    avg_loss = running_loss / num_batches\n",
    "\n",
    "    #calcular metrica total: promedio de metricas de cada batch\n",
    "    metric_results = metrics.compute()\n",
    "\n",
    "    #crear diccionario de resutaldos: primer elemento del diccionario es el loss\n",
    "    results = {\"val_loss\": avg_loss}\n",
    "    #añadir elementos al diccionario: cada par clave-valor es una métrica\n",
    "    results.update({k: v.item() for k, v in metric_results.items()})\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Epoch {epoch} - Val   | Loss: {avg_loss:.4f} | Time: {elapsed_time:.1f}s\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos que no hay inconsistencias, a pesar de que los pesos de los modelos sean aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Val   | Loss: 81.9369 | Time: 0.6s\n",
      "{'val_loss': 81.93691555658977, 'MAE': 7.372611999511719, 'MAPE': 28434.85546875, 'MSE': 82.05606079101562, 'RMSE': 9.058480262756348}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "val_results = validate_one_epoch(\n",
    "    model=models[\"lstm\"][6],\n",
    "    dataloader=val_dataloaders[6],\n",
    "    criterion=criterions[\"lstm\"][6],\n",
    "    metrics = metrics,\n",
    "    device= DEVICE,\n",
    "    epoch=1\n",
    ")\n",
    "\n",
    "print(val_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Definir bucle de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "                model,\n",
    "                dataloader,\n",
    "                criterion,\n",
    "                optimizer,\n",
    "                metrics,\n",
    "                device,\n",
    "                epoch = None\n",
    "):\n",
    "        \"\"\"\n",
    "        Ejecuta una epoch completa de entrenamiento.\n",
    "\n",
    "        Args:\n",
    "                model (nn.Module): Modelo a entrenar.\n",
    "                dataloader (DataLoader): Dataloader con datos de entrenamiento.\n",
    "                criterion (nn.Module): Función de pérdida.\n",
    "                optimizer (Optimizer): Optimizador para actualizar pesos.\n",
    "                metrics (torchmetrics.MetricCollection): Métricas de evaluación.\n",
    "                device (str): \"cpu\" o \"cuda\".\n",
    "                epoch (int, optional): Número de la época actual (solo para mostrar).\n",
    "\n",
    "        Returns:\n",
    "                dict: Diccionario con loss promedio y métricas acumuladas.\n",
    "        \"\"\"\n",
    "        \n",
    "        model.train() #poner modelo en modo entrenamiento\n",
    "\n",
    "        #inicializar loss y nº de batches\n",
    "        running_loss = 0.0\n",
    "        num_batches = len(dataloader)\n",
    "\n",
    "        #inicializar métricas y moverlas al device\n",
    "        metrics.to(device)\n",
    "        metrics.reset()\n",
    "\n",
    "        #coger tiempo actual de referencia\n",
    "        start_time = time.time()\n",
    "\n",
    "        #inicializar barra de progreso\n",
    "        #progress_bar envuelve el dataloader y lo monitoriza\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch}\", leave=False)\n",
    "\n",
    "\n",
    "        for x_batch, y_batch in progress_bar:\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                optimizer.zero_grad() #1. resetear gradientes\n",
    "\n",
    "                y_pred = model(x_batch) #2. Forward\n",
    "\n",
    "                loss = criterion(y_pred, y_batch) # 3. calcular loss\n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                batch_idx = progress_bar.n #indice del batch\n",
    "                avg_loss_so_far = running_loss / (batch_idx + 1)\n",
    "\n",
    "                #Mostrar en tiempo real el loss\n",
    "                progress_bar.set_postfix({\"Loss\": avg_loss_so_far})\n",
    "\n",
    "\n",
    "                loss.backward() #4. Propagar loss\n",
    "                optimizer.step() #5. Actualizar pesos\n",
    "\n",
    "\n",
    "                metrics.update(y_pred, y_batch)\n",
    "        \n",
    "        #calcular loss promedio del batch\n",
    "        avg_loss = running_loss/num_batches\n",
    "\n",
    "        #calcular metrica total: promedio de metricas de cada batch\n",
    "        metric_results = metrics.compute()\n",
    "\n",
    "        #crear diccionario de resutaldos: primer elemento del diccionario es el loss\n",
    "        results = {\"train_loss\": avg_loss}\n",
    "\n",
    "        #añadir elementos al diccionario: cada par clave-valor es una métrica\n",
    "        results.update({k: v.item() for k,v in metric_results.items()})\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Epoch {epoch} - Train | Loss: {avg_loss:.4f} | Time: {elapsed_time:.1f}s\")\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos que no hay inconsistencias, a pesar de que los pesos de los modelos sean aleatorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train | Loss: 93.8941 | Time: 6.1s\n",
      "{'train_loss': 93.89411600924086, 'MAE': 7.749772548675537, 'MAPE': 3310.0859375, 'MSE': 93.90457153320312, 'RMSE': 9.690437316894531}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "train_results = train_one_epoch(\n",
    "    model=models[\"lstm\"][6],\n",
    "    dataloader=train_dataloaders[6],\n",
    "    criterion=criterions[\"lstm\"][6],\n",
    "    optimizer=optimizers[\"lstm\"][6],\n",
    "    metrics=metrics,\n",
    "    device=DEVICE,\n",
    "    epoch = 1\n",
    ")\n",
    "\n",
    "print(train_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Optimizació óptima de hiperparámetros con `Optuna`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    metrics,\n",
    "    scheduler=None,\n",
    "    num_epochs=10,\n",
    "    device=\"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Entrena un modelo durante varias épocas y evalúa en validación.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo a entrenar.\n",
    "        train_loader (DataLoader): Dataloader de entrenamiento.\n",
    "        val_loader (DataLoader): Dataloader de validación.\n",
    "        criterion (nn.Module): Función de pérdida.\n",
    "        optimizer (Optimizer): Optimizador.\n",
    "        metrics (torchmetrics.MetricCollection): Métricas compartidas entre train/val.\n",
    "        scheduler (optional): Scheduler de LR.\n",
    "        num_epochs (int): Número de épocas.\n",
    "        device (str): \"cpu\" o \"cuda\".\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de dicts con métricas de cada época.\n",
    "    \"\"\"\n",
    "\n",
    "    history = []  # Para guardar métricas por época\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "\n",
    "        # Entrenamiento\n",
    "        train_metrics = train_one_epoch(\n",
    "            model=model,\n",
    "            dataloader=train_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            metrics=metrics.clone(),  # ← importante: evita conflictos\n",
    "            device=device,\n",
    "            epoch=epoch\n",
    "        )\n",
    "\n",
    "        # Validación\n",
    "        val_metrics = validate_one_epoch(\n",
    "            model=model,\n",
    "            dataloader=val_loader,\n",
    "            criterion=criterion,\n",
    "            metrics=metrics.clone(),\n",
    "            device=device,\n",
    "            epoch=epoch\n",
    "        )\n",
    "\n",
    "        # Scheduler (opcional)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Unir métricas\n",
    "        combined = {\"epoch\": epoch}\n",
    "        combined.update(train_metrics)\n",
    "        combined.update(val_metrics)\n",
    "        history.append(combined)\n",
    "\n",
    "        # Mostrar resumen\n",
    "        print(\n",
    "            f\"📊 Summary | Train Loss: {train_metrics['train_loss']:.4f} | \"\n",
    "            f\"Val Loss: {val_metrics['val_loss']:.4f}\"\n",
    "        )\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train | Loss: 14.9539 | Time: 6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Val   | Loss: 16.3196 | Time: 0.8s\n",
      "📊 Summary | Train Loss: 14.9539 | Val Loss: 16.3196\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train | Loss: 14.9239 | Time: 6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Val   | Loss: 16.4341 | Time: 0.6s\n",
      "📊 Summary | Train Loss: 14.9239 | Val Loss: 16.4341\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train | Loss: 14.9418 | Time: 6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Val   | Loss: 16.3599 | Time: 0.6s\n",
      "📊 Summary | Train Loss: 14.9418 | Val Loss: 16.3599\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train | Loss: 14.9173 | Time: 6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Val   | Loss: 16.4595 | Time: 0.6s\n",
      "📊 Summary | Train Loss: 14.9173 | Val Loss: 16.4595\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Train | Loss: 14.9104 | Time: 6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Val   | Loss: 16.3092 | Time: 0.6s\n",
      "📊 Summary | Train Loss: 14.9104 | Val Loss: 16.3092\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Train | Loss: 14.9152 | Time: 5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Val   | Loss: 16.3922 | Time: 0.6s\n",
      "📊 Summary | Train Loss: 14.9152 | Val Loss: 16.3922\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Train | Loss: 14.9080 | Time: 5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Val   | Loss: 16.3527 | Time: 0.6s\n",
      "📊 Summary | Train Loss: 14.9080 | Val Loss: 16.3527\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Train | Loss: 14.9048 | Time: 6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Val   | Loss: 16.2733 | Time: 0.6s\n",
      "📊 Summary | Train Loss: 14.9048 | Val Loss: 16.2733\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Train | Loss: 14.9064 | Time: 5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Val   | Loss: 16.4183 | Time: 0.6s\n",
      "📊 Summary | Train Loss: 14.9064 | Val Loss: 16.4183\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Train | Loss: 14.9069 | Time: 6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Val   | Loss: 16.3308 | Time: 0.6s\n",
      "📊 Summary | Train Loss: 14.9069 | Val Loss: 16.3308\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Train | Loss: 14.8986 | Time: 6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Val   | Loss: 16.2677 | Time: 0.6s\n",
      "📊 Summary | Train Loss: 14.8986 | Val Loss: 16.2677\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Train | Loss: 14.8944 | Time: 6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Val   | Loss: 16.2855 | Time: 0.6s\n",
      "📊 Summary | Train Loss: 14.8944 | Val Loss: 16.2855\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Train | Loss: 14.8972 | Time: 6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Val   | Loss: 16.2984 | Time: 0.6s\n",
      "📊 Summary | Train Loss: 14.8972 | Val Loss: 16.2984\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Train | Loss: 14.8934 | Time: 6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Val   | Loss: 16.2335 | Time: 0.6s\n",
      "📊 Summary | Train Loss: 14.8934 | Val Loss: 16.2335\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Train | Loss: 14.9026 | Time: 6.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Val   | Loss: 16.2823 | Time: 0.8s\n",
      "📊 Summary | Train Loss: 14.9026 | Val Loss: 16.2823\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Train | Loss: 14.8924 | Time: 6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - Val   | Loss: 16.2687 | Time: 0.6s\n",
      "📊 Summary | Train Loss: 14.8924 | Val Loss: 16.2687\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Train | Loss: 14.8951 | Time: 6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - Val   | Loss: 16.2463 | Time: 0.6s\n",
      "📊 Summary | Train Loss: 14.8951 | Val Loss: 16.2463\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Train | Loss: 14.8930 | Time: 6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - Val   | Loss: 16.2183 | Time: 0.6s\n",
      "📊 Summary | Train Loss: 14.8930 | Val Loss: 16.2183\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Train | Loss: 14.8904 | Time: 5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - Val   | Loss: 16.2984 | Time: 0.6s\n",
      "📊 Summary | Train Loss: 14.8904 | Val Loss: 16.2984\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Train | Loss: 14.9233 | Time: 5.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - Val   | Loss: 16.2800 | Time: 0.6s\n",
      "📊 Summary | Train Loss: 14.9233 | Val Loss: 16.2800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "history = fit(\n",
    "    model=models[\"lstm\"][6],\n",
    "    train_loader=train_dataloaders[6],\n",
    "    val_loader=val_dataloaders[6],\n",
    "    criterion=criterions[\"lstm\"][6],\n",
    "    optimizer=optimizers[\"lstm\"][6],\n",
    "    scheduler=schedulers[\"lstm\"][6],\n",
    "    metrics=metrics,  # una instancia de MetricCollection\n",
    "    num_epochs=20,\n",
    "    device=DEVICE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Predicciones sobre el conjunto de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, dataloader, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Obtiene las predicciones del modelo sobre un dataloader completo.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): Modelo entrenado.\n",
    "        dataloader (DataLoader): Dataloader a predecir (train, val o test).\n",
    "        device (str): \"cpu\" o \"cuda\".\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tensor, Tensor]: (y_true, y_pred), ambos de tamaño (N, output_size)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            y_pred = model(x_batch)\n",
    "\n",
    "            all_preds.append(y_pred.cpu())\n",
    "            all_targets.append(y_batch.cpu())\n",
    "\n",
    "    y_pred = torch.cat(all_preds, dim=0)\n",
    "    y_true = torch.cat(all_targets, dim=0)\n",
    "\n",
    "    return y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2758, 1]) torch.Size([2758, 1])\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred = predict(\n",
    "    model=models[\"lstm\"][6],\n",
    "    dataloader=test_dataloaders[6],\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "print(y_true.shape, y_pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[62.9032],\n",
       "        [59.1398],\n",
       "        [59.4086],\n",
       "        [59.4086],\n",
       "        [59.1398]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[67.1369],\n",
       "        [53.9262],\n",
       "        [55.5845],\n",
       "        [59.7052],\n",
       "        [58.4262]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Exportación de checkpoints y logs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TS-PARKINGS_VENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
